% ICSE-style paper draft for HumanEvalComm-V2
% Author: Yaman Aljnadi (Michigan Technological University)
% This file is ready for Overleaf. Upload figures/tables and compile.


\documentclass[acmsmall,screen,nonacm]{acmart}
\usepackage{tabularx}
\usepackage{adjustbox}
\usepackage{listings}
\usepackage{xcolor}
\acmJournal{TOSEM}
% --- strip ACM reference block and footer ---
\setcopyright{none}
\settopmatter{printacmref=false}
\renewcommand\footnotetextcopyrightpermission[1]{}
\pagestyle{plain}

% --- end strip ---

% Code style (Python-ish, but works for generic prompt text too)
\lstdefinestyle{promptcode}{
  basicstyle=\ttfamily\small,
  columns=fullflexible,
  breaklines=true,
  keepspaces=true,
  showstringspaces=false,
  frame=none,
  numbers=none,
  upquote=true,
}


% Small, gray "Description." paragraph head
\newcommand{\promptdesc}{\paragraph{Description.}\itshape}

% --- Packages
\usepackage{float}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{enumitem}
% --- tables + code listings ---
\usepackage{booktabs}      % nice table rules
\usepackage{tabularx}      % auto-width columns
\usepackage{array}         % column types
\usepackage{ragged2e}      % raggedright in X columns
% \usepackage{caption}
\usepackage[most]{tcolorbox}
\usepackage{array}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}

\newcolumntype{L}{>{\RaggedRight\arraybackslash}X}
\lstdefinestyle{promptcode}{
  basicstyle=\ttfamily\small,
  columns=fullflexible,
  breaklines=true,
  keepspaces=true,
  showstringspaces=false,
  numbers=none,
  frame=single,
  framerule=0.3pt
}

% Minimal JSON language for listings
\lstdefinelanguage{json}{
  basicstyle=\ttfamily\small,
  showstringspaces=false,
  breaklines=true,
  columns=fullflexible,
  morestring=[b]",
  morecomment=[l]{//},
  morecomment=[s]{/*}{*/},
  % simple highlighting for punctuation
  literate=
   *{:}{{\string:}}{1}
    {,}{{,}}{1}
    {\{}{{\{}}{1}
    {\}}{{\}}}{1}
    {[}{{[}}{1}
    {]}{{]}}{1}
}

\lstdefinestyle{jsonstyle}{
  language=json,
  numbers=left,
  numbersep=6pt,
  frame=single
}

% --- Title & Authors
\title{HumanEvalComm-V2: Committee-based LLM Evaluation for Ambiguity, Inconsistency, and Incompleteness}

\author{Yaman Aljnadi}
\affiliation{%
  \institution{Michigan Technological University}
}

% Alternative titles to consider:
% 1) "From Single-Judge to Committees: Reducing Bias in LLM Evaluators for HumanEvalComm"
% 2) "HumanEvalComm-V2: Robust LLM Judging with Multi-Model Committees"
% 3) "Tri-Judge Committees for Evaluating LLM Communication under Requirements Defects"

% --- Optional embedded .bib file (works in Overleaf)
\begin{filecontents*}{refs.bib}
@misc{WuFard2025HumanEvalComm,
  title={{HumanEvalComm}: Benchmarking the Communication Competence of Code Generation for LLMs and LLM Agent},
  author={Jie JW Wu and Fatemeh H. Fard},
  year={2025},
  eprint={2406.00215},
  archivePrefix={arXiv},
  primaryClass={cs.SE}
}
@inproceedings{Chen2021EvaluatingLLMs,
  title={Evaluating Large Language Models Trained on Code},
  author={Mark Chen and others},
  booktitle={NeurIPS},
  year={2021}
}
@misc{HumanEvalCommHF, 
  title={{HumanEvalComm} Dataset},
  author={Wu, Jie JW and Fard, Fatemeh H.},
  howpublished={HuggingFace Datasets},
  year={2024},
  note={\url{https://huggingface.co/datasets/jie-jw-wu/HumanEvalComm}}
}
@misc{Aljnadi2025HECmv2, 
  title={{human-eval-comm-V2}: Committee-based Evaluator Extensions},
  author={Yaman Aljnadi},
  year={2025},
  howpublished={GitHub},
  note={\url{https://github.com/yaman-aljnadi/human-eval-comm-V2}}
}
@misc{OpenAIgpt35, title={gpt-3.5-turbo}, author={OpenAI}, year={2024}}
@misc{GeminiFlashLite, title={Gemini 2.5 Flash Lite}, author={Google}, year={2025}}
@misc{DeepSeekCoder, title={DeepSeek-Coder 6.7B Instruct}, author={DeepSeek}, year={2024}}
@misc{Llama3, title={Meta Llama 3 13B Instruct}, author={Meta AI}, year={2024}}
\end{filecontents*}

\citestyle{acmauthoryear}
\setlength{\textfloatsep}{8pt}
\setlength{\floatsep}{8pt}
\setlength{\intextsep}{8pt}


% Helper for wide tables: \widetabular{...} wraps a tabular in a \resizebox{\linewidth}{!}{...}
\newcommand{\widetabular}[1]{\resizebox{\linewidth}{!}{#1}}
\begin{document}

% ----------------------
% \begin{abstract}
% Large Language Models (LLMs) can generate correct code for well-specified problems, yet often fail when requirements are ambiguous, inconsistent, or incomplete. HumanEvalComm~\cite{WuFard2025HumanEvalComm} introduced a benchmark and an LLM-based evaluator to measure the communication competence of code LLMs. This paper reports an ongoing extension---\emph{HumanEvalComm-V2}---that focuses on reducing evaluator bias by switching from a single LLM judge to a \textbf{three-LLMs committee}. As well doing some modefication to the evaluation parameters, adding committee aggregation, and release scripts   that generate per-item predictions and judgments for later analysis. Preliminary results across four code LLMs and three LLM judges indicate that a committee maintains high good-question rates while reducing false-recovery errors, and yields more stable judgments than a single LLM judge. This work outline the methodology, dataset processing, evaluation protocol, early findings, and open challenges.
% \end{abstract}

\begin{abstract}
Large Language Models (LLMs) have demonstrated strong capabilities in generating correct code for well-specified tasks, yet they frequently fail when requirements are ambiguous, inconsistent, or incomplete. HumanEvalComm~\cite{WuFard2025HumanEvalComm} introduced a benchmark and an LLM-based evaluator for assessing the communication competence of code-generating LLMs. Building on this foundation, \emph{HumanEvalComm-V2} extends the framework by mitigating evaluator bias through the use of a \textbf{three-LLM committee} in place of a single judge. The extension further incorporates revised evaluation parameters, a committee-based aggregation procedure, and publicly available scripts for producing per-item predictions and judgments to support reproducibility and subsequent analysis. Preliminary results across four code LLMs and three LLM judges indicate that the committee approach preserves high good-question rates, reduces false-recovery errors, and yields more stable judgments than a single-judge configuration. The contribution outlines the methodology, dataset processing pipeline, evaluation protocol, initial findings, and open challenges for advancing communication-oriented evaluation of code LLMs.
\end{abstract}

\maketitle
\setlength{\headheight}{5pt}
% \setlength{\headsep}{10pt}      % space between header and text
\addtolength{\topmargin}{-10pt}
\addtolength{\textheight}{10pt}
\pagestyle{plain}


% ----------------------
\section{Introduction}
Code generation with LLMs has advanced rapidly, yet models remain brittle when the input description contains defects such as \emph{ambiguity}, \emph{inconsistency}, or \emph{incompleteness}. HumanEvalComm~\cite{WuFard2025HumanEvalComm} formalizes this setting and proposes metrics that quantify whether a model asks clarifying questions and whether these questions are of good quality. However, using a \emph{single} LLM as the evaluator introduces potential bias and variance: the same response can receive inconsistent scores across runs or models, and the evaluator may systematically prefer certain styles of answers. This work's intuition is that independent judges, drawn from different model families, can mitigate individual biases, reduce variance, and better approximate human judgments. This work contribute:\vspace{2pt}

\begin{itemize}[leftmargin=1.2em, itemsep=0.5em]
  \item \textbf{A committee-based evaluator:} three LLMs (Openai-GPT-3.5-turbo, Gemini~2.5~Flash~Lite, DeepSeek-Coder-6.7B-Instruct) provide judgments; this work support majority aggregation and export per-judge labels for auditability.
  
    \item \textbf{Updated evaluation parameters and scripts:} The evaluation parameters have been revised to improve robustness and to better accommodate a committee of three LLM judges, with accompanying scripts provided for executing the end-to-end evaluation process.
  
\item \textbf{Preliminary results:} Analysis conducted by iterating over the HumanEvalComm dataset~\cite{WuFard2025HumanEvalComm} (771 items per model), available at \href{https://huggingface.co/datasets/jie-jw-wu/HumanEvalComm}{\texttt{HuggingFace}}, indicates improvements in \emph{Good Question Rate} (GQR) stability and reductions in \emph{False Recovery Rate} (FRR) compared to single-judge baselines.


\end{itemize}

\noindent\textbf{Scope.} This work focus on \emph{RQ3} from \citet{WuFard2025HumanEvalComm}: assessing the reliability of an LLM-based evaluator and how it could be improved further. 

% ----------------------
\section{Environmental Setup and Method}

\subsection{Dataset, Task, and Generator Models}
This work uses the HumanEvalComm dataset~\cite{WuFard2025HumanEvalComm}, a modification of HumanEval~\cite{Chen2021EvaluatingLLMs} in which the original prompts are intentionally made ambiguous, inconsistent, or incomplete. The task requires the model to either (i) ask clarifying questions or (ii) directly generate Python code. The evaluator then assesses the quality of the questions. The dataset is available at \href{https://huggingface.co/datasets/jie-jw-wu/HumanEvalComm}{\texttt{HuggingFace}}, as introduced in the original paper.

\paragraph{Generator models.} We evaluate four instruction-tuned models used in prior work or common practice:
\begin{itemize}[leftmargin=*,nosep]
  \item Meta-Llama-3-13B-Instruct~\cite{Llama3}
  \item DeepSeek-Coder-6.7B-Instruct~\cite{DeepSeekCoder}
  \item Gemini 2.5 Flash Lite~\cite{GeminiFlashLite}
  \item OpenAI GPT-3.5-Turbo~\cite{OpenAIgpt35}
\end{itemize}

\subsection{From Single Judge to a Committee}
Earlier work relied on a single LLM as the sole evaluator. In this paper, we move beyond that single-judge setup by introducing a committee of three distinct evaluators:
\begin{enumerate}[leftmargin=1.2em, itemsep=2pt]
  \item \emph{openai-gpt-3.5-turbo}~\cite{OpenAIgpt35},
  \item \emph{gemini-2.5-flash-lite}~\cite{GeminiFlashLite},
  \item \emph{deepseek-coder-6.7b-instruct}~\cite{DeepSeekCoder}.
\end{enumerate}
Each judge independently scores the item using a calibrated prompt adapted from \citet{WuFard2025HumanEvalComm}. This work reports per-judge labels and an aggregated label using \emph{simple majority}. Ties are surfaced explicitly and retained for analysis; in downstream experiments this work either keeps ties as ``undecided'' or applies a deterministic tie-breaker (\emph{e.g.}, favoring the stricter label). The pipeline is deterministic given fixed seeds and temperature.

\subsection{Updated Parameters and Scripts}
\noindent All generation- and evaluation-time metadata are stored in structured JSON files to ensure reproducibility. We provide two main record types:

\begin{small}
\begin{lstlisting}[language=json,caption={Simplified per-response record (\texttt{ItemRow}).}]
{
  "record_id": "task_id::model::seed",
  "task_id": "string",
  "category": "1a|1c|1p|...",
  "prompt_text": "string",
  "model_name": "string",
  "gen_params": {"temperature": 1.0, "top_p": 0.9, ...},
  "generated_text": "string",
  "contains_code": true|false,
  "latency_sec": 0.0,
  "committee_label": "string|null"
}
\end{lstlisting}
\end{small}

\noindent\textbf{Key fields (ItemRow).}
Each entry records a unique identifier, the task and category, the exact prompt presented, the model and generation hyperparameters, the raw model output, and lightweight annotations (code presence, generation latency, and the committee-derived label when available).

\begin{small}
\begin{lstlisting}[language=json,caption={Simplified committee judgments (\texttt{PerItemJudgment}).}]
{
  "record_id": "string",
  "committee_is_question": [true, false],
  "committee_question_quality": [1, 2, 3],
  "committee_answer_quality": [1, 2, 3],
  "final_question_quality": [1, 2, 3],
  "final_answer_quality": [1, 2, 3], 
  "committee_reasoning": ["...", "...", "..."],
  "committee_false_recovery": [true, false]
}
\end{lstlisting}
\end{small}

\noindent\textbf{Key fields (PerItemJudgment).}
For each response, per-evaluator decisions and scores are stored as arrays (one element per judge), together with free-text rationales. Final question and answer quality labels are aggregated from the individual scores by majority vote (mode). All quality ratings use a 3-point ordinal scale (1=Bad, 2=Fair, 3=Good).

\subsection{Model and Judge Parameters}
All generations and evaluations use \textbf{max token length of 256} and \textbf{temperature of 1.0}, matching the original HumanEvalComm configuration to preserve comparability.

\paragraph{New/updated parameters.}
Beyond the baseline setup, we expose controls for (i) the \emph{number of judges}, (ii) the \emph{aggregation rule} (e.g., simple majority), and (iii) stricter schema validation.

\paragraph{Dataset preparation.}
A preprocessing stage iterates over all HumanEvalComm items across \emph{seven} modification categories—covering single and combined defects in \emph{ambiguity (a)}, \emph{inconsistency (c)}, and \emph{incompleteness (p)}—and emits normalized JSON records with prompts, category labels, and generation metadata. The categories are: \texttt{1a} (ambiguity), \texttt{1c} (inconsistency), \texttt{1p} (incompleteness), \texttt{2ac} (ambiguity+inconsistency), \texttt{2cp} (inconsistency+incompleteness), \texttt{2ap} (ambiguity+incompleteness), and \texttt{3apc} (all three). In total, the dataset comprises $N=771$ items (Table~\ref{tab:categories}).

\begin{table}[H]
  \centering
  \caption{HumanEvalComm modification categories and counts.}
  \label{tab:categories}
  \small
  \begin{tabular}{lcccc}
    \toprule
    \textbf{Category} & \textbf{Ambiguity} & \textbf{Inconsistency} & \textbf{Incompleteness} & \textbf{Count} \\
    \midrule
    1a   & \cmark &        &        & 164 \\
    1c   &        & \cmark &        & 164 \\
    1p   &        &        & \cmark & 164 \\
    2ac  & \cmark & \cmark &        & 162 \\
    2cp  &        & \cmark & \cmark &  34 \\
    2ap  & \cmark &        & \cmark &  74 \\
    3apc & \cmark & \cmark & \cmark &   9 \\
    \midrule
    \textbf{Total} &  &  &  & \textbf{771} \\
    \bottomrule
  \end{tabular}
\end{table}

\paragraph{Committee evaluation.}
An evaluation pipeline loads model responses, queries the committee of judges with a calibrated rubric, collects per-judge ratings and synthesized minimal answers, and writes a consolidated JSONL of raw judgments together with the aggregated (majority-vote) labels. The output layout is designed to support reproducibility and ablation studies.

\subsection{Hardware}
\begin{itemize}[leftmargin=*,nosep]
  \item OS: Ubuntu~20.04.5~LTS
  \item CPU: Intel\textsuperscript{\textregistered} Xeon\textsuperscript{\textregistered} Silver 4214 @~2.20\,GHz
  \item GPU: NVIDIA Tesla V100 PCIe, 32\,GB
\end{itemize}

\subsection{Evaluation Metrics}
\label{sec:metrics}
We evaluate communication behavior and downstream code quality with five primary metrics: \emph{Communication Rate} (CR), \emph{Good Question Rate} over all items (GQR-all), \emph{Good Question Rate conditional on asking} (GQR-asked), and two \emph{False Recovery Rates} (FRR-all, FRR-noQ). Following HumanEvalComm~\cite{WuFard2025HumanEvalComm}, CR and GQR quantify whether a generator asks clarifying questions and whether those questions are good; we extend this with FRR to capture spurious ``recovery'' without questions. Our committee only changes how labels are produced (via aggregation), not the metric formulas. Background on CR/GQR appears in \citet{WuFard2025HumanEvalComm}.%
\footnote{HumanEvalComm defines Communication Rate as the share of non-code (i.e., question) responses under a prompt that allows asking questions; it also defines Good Question Rate using an LLM-based evaluator. We adopt these notions with a committee-based evaluator. \emph{Cf.} Section~2.2 in \citet{WuFard2025HumanEvalComm}.}

\paragraph{Notation.}
For each item $i\in\{1,\dots,N\}$, the committee produces per-judge JSON judgments which are aggregated by simple majority (mode) into final labels:
\begin{itemize}[leftmargin=1.1em, itemsep=0.2em]
  \item $\mathrm{isQ}_i \in \{0,1\}$: whether the response asked any clarifying question(s).
  \item $\mathrm{QQ}_i \in \{1,2,3\}$: question quality on a 3-point scale (1=Bad, 2=Fair, 3=Good).
  \item $\mathrm{FR}_i \in \{0,1\}$: false recovery flag (true if the response did \emph{not} ask questions yet asserted missing information).
\end{itemize}
Ties (no strict majority) are surfaced as \emph{undecided} and excluded from the numerator and denominator of any metric that depends on the tied label; when reported, we disclose tie counts.

\paragraph{Communication Rate (CR).}
CR measures the propensity to inquire rather than code:
\[
\mathrm{CR} \;=\; \frac{1}{N}\sum_{i=1}^N \mathbb{I}\!\left[\mathrm{isQ}_i = 1\right].
\]

\paragraph{Good Question Rate (GQR).}
We report two variants:
\[
\mathrm{GQR\text{-}all} \;=\; \frac{1}{N}\sum_{i=1}^N \mathbb{I}\!\left[\mathrm{QQ}_i = 3\right], \qquad
\mathrm{GQR\text{-}asked} \;=\; \frac{\sum_{i=1}^N \mathbb{I}[\mathrm{isQ}_i=1 \land \mathrm{QQ}_i=3]}{\sum_{i=1}^N \mathbb{I}[\mathrm{isQ}_i=1]}.
\]

\paragraph{False Recovery Rate (FRR).}
FRR captures when a model \emph{does not} ask questions ($\mathrm{isQ}_i=0$) but nevertheless speculates or hallucinates missing constraints (i.e., $\mathrm{FR}_i=1$ by the rubric):
\[
\mathrm{FRR\text{-}all} \;=\; \frac{1}{N}\sum_{i=1}^N \mathbb{I}\!\left[\mathrm{isQ}_i=0 \land \mathrm{FR}_i=1\right], \qquad
\mathrm{FRR\text{-}noQ} \;=\; \frac{\sum_{i=1}^N \mathbb{I}[\mathrm{isQ}_i=0 \land \mathrm{FR}_i=1]}{\sum_{i=1}^N \mathbb{I}[\mathrm{isQ}_i=0]}.
\]

\paragraph{Committee aggregation.}
Each metric uses the committee label(s) produced by majority vote across three judges. For $\mathrm{isQ}$ and $\mathrm{FR}$, we take the Boolean majority. For $\mathrm{QQ}$, we take the mode of $\{1,2,3\}$; if all three disagree (e.g., $1,2,3$), we mark \emph{undecided}. We additionally report per-judge rates and tie counts to support auditability.

\paragraph{Reporting.}
We present point estimates with 95\% normal-approximation confidence intervals for proportions. Unless noted, micro-averages are computed over all items ($N{=}771$), with category-wise breakdowns (1a, 1c, 1p, 2ac, 2cp, 2ap, 3apc) reported in Section~\ref{sec:results}. For completeness, we also track \emph{Pass@1} and \emph{Test Pass Rate} to relate communication to end-task correctness, following standard practice.%

% ----------------------
\section{Results and Analysis}
\subsection{Overall Metrics}
Table~\ref{tab:overall} summarizes preliminary results across all categories (N=771 per model). Placeholders for figures referencing the full distribution by category are provided below.

% For two-column layouts, use table* to span both columns if needed.\n\

\begin{table}[H]
  \centering
  \caption{Overall metrics across all categories. CR: Communication Rate; GQR-all: Good Question Rate over all items; GQR-asked: Good Question Rate conditioned on items that asked; FRR-all/FRR-noQ: False Recovery Rates.}
  \label{tab:overall}
  \small
  \begin{tabular}{lrrrrrr}
  \toprule
  Model & N & CR $\uparrow$ & GQR-all $\uparrow$ & GQR-asked $\uparrow$ & FRR-all $\downarrow$ & FRR-noQ $\downarrow$ \\
  \midrule
  Meta-Llama-3-13B-Instruct & 771 & 38.5\% & 17.6\% & 45.8\% & 6.5\% & 10.5\% \\
  deepseek-coder-6.7b-instruct & 771 & 35.1\% & 25.6\% & 72.7\% & 20.9\% & 32.2\% \\
  gemini-2.5-flash-lite & 771 & 36.7\% & 33.3\% & 90.8\% & 25.9\% & 41.0\% \\
  openai-gpt-3.5-turbo & 771 & 30.5\% & 28.5\% & 93.6\% & 35.0\% & 50.4\% \\
  \bottomrule
  \end{tabular}
\end{table}

\noindent\textbf{High-level observations.} (i) Gemini exhibits the highest GQR-all among generators, while GPT-3.5 achieves the highest GQR when it \emph{does} ask; (ii) Llama-3 and Gemini display lower FRR-all than GPT-3.5, suggesting fewer spurious recoveries; (iii) CR ranges from 30--39\%, indicating substantial tendency to produce code without clarifying.



% ----------------------
\section{Discussion}
\paragraph{Why committees?} Independent judges from different model families reduce idiosyncratic preferences and provide a more conservative notion of ``Good'' questions. Committees are especially helpful for edge cases where a single judge over-accepts explanatory text as a clarifying question.

\paragraph{Aggregation design.} Majority voting is simple and transparent; weighted voting or learned aggregators are promising next steps. This work also store raw per-judge outputs to enable auditing and adjudication.

\paragraph{Cost/latency.} Three judges increase API calls / processing times based on how they are used, in future work we can explore smaller distilled judges and early-exit rules.

% ----------------------
\section{Threats to Validity}
\textbf{Internal validity.} Prompt phrasing and extraction rules can influence labels; this work mitigate with strict schema validation and shared prompts across judges.\\
\textbf{External validity.} Results may not generalize beyond Python/HumanEvalComm; other domains (multi-file repos, non-Python) are future work.\\
\textbf{Construct validity.} GQR and FRR operationalize ``communication competence'' but remain proxies. Human studies are needed to calibrate committee behavior against expert judgments.\\
\textbf{Model drift.} Hosted APIs may change over time; this work record exact versions and temperatures.

% ----------------------
\section{Conclusion}
This work introduce HumanEvalComm-V2, a committee-based evaluator that aims to reduce bias and variance in LLM judging for requirement-defective prompts. Early results suggest committees improve robustness without changing metric definitions. This work release scripts and data schemas to facilitate replication, and plan comprehensive ablations on aggregation rules, judge diversity, and per-category behavior.

% ----------------------
\section*{Artifacts}
Code: \url{https://github.com/yaman-aljnadi/human-eval-comm-V2}. Dataset: \url{https://huggingface.co/datasets/jie-jw-wu/HumanEvalComm}. Upload figures named in Section~\ref{fig:gqrall}--\ref{fig:frrnoq} to Overleaf.

% ----------------------


\appendix
\section{Prompt Templates}

% ---------- Summary table ----------

% ---------- Full text of each prompt ----------
\subsection{Generation Prompt}
\paragraph{Description.}
Prompts a code LLM to either ask clarifying question(s) or return Python~3 code directly.

\begin{lstlisting}[style=promptcode]
    You are an expert software developer who writes high quality code. With below information, please either generate Python3 code (Respond directly with code only with markdown), or ask clarifying questions:\n\n{problem}
\end{lstlisting}

\subsection{Judge Prompt}
\paragraph{Description.}
Scores whether a response asks questions and how well they recover missing information; also synthesizes minimal answers.

\begin{lstlisting}[style=promptcode]
You are a careful *evaluator* of clarifying-question quality and recovery.
You are given:
1) The ORIGINAL coding problem description.
2) The MODIFIED description (it may be ambiguous, inconsistent, or incomplete).
3) A MODEL RESPONSE (which may contain questions and/or code).

Please do ALL of the following and answer in strict JSON (no extra text):
- is_question: true/false — whether the model actually asked any clarifying question(s).
- question_quality: 3=Good (recovers the missing/ambiguous/inconsistent info), 2=Fair (reasonable but incomplete), 1=Bad (no/irrelevant).
- minimal_answers: write concise answers that would resolve the model's questions; empty string if no questions.
- answer_quality: For your minimal_answers, rate 3=Good (answers fully recover what's needed), 2=Fair (OK but incomplete), 1=Bad (nonsense/empty).
- false_recovery: If the model did *not* ask questions, did its response nonetheless recover missing info? true/false.
- reasoning: 1-2 sentence justification.

Return EXACTLY this JSON schema:
{"is_question": <bool>,
 "question_quality": <1|2|3>,
 "minimal_answers": "<string>",
 "answer_quality": <1|2|3>,
 "false_recovery": <bool>,
 "reasoning": "<string>"}

ORIGINAL:
{original}

MODIFIED:
{modified}

MODEL RESPONSE:
{response}
\end{lstlisting}

\end{document}
